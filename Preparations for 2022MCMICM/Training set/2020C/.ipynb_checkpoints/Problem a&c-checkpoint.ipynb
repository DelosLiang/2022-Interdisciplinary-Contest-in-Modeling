{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9f4c1c",
   "metadata": {},
   "source": [
    "## 1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b4638",
   "metadata": {},
   "source": [
    "### 1.1 Problem Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a086c9",
   "metadata": {},
   "source": [
    "### 1.2 Clarifications and Reatatements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ee075",
   "metadata": {},
   "source": [
    "### 1.3 Our Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b529ef3",
   "metadata": {},
   "source": [
    "## 2 Reasonable Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d52d2",
   "metadata": {},
   "source": [
    "To simplify our model and eliminate the complexity, we make the following main assumptions in the literature. All assumptions will be re-emphasized once they are used in the construction of our model.\n",
    "\n",
    "\n",
    "**Assumption 1.** There is highly linear positive correlation between sales volume and the number of reviews.\n",
    "\n",
    "\n",
    "**Assumption 2.** The content of the comments is credible and unbiased.\n",
    "\n",
    "\n",
    "**Assumption 3.**  The comments the users have initially seen are the latest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bcdc04",
   "metadata": {},
   "source": [
    "### 2.1 Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89591d",
   "metadata": {},
   "source": [
    "In the process of examing the data set, we found out that it is not pure as expected.For instance, it contains irrelevant kinds of products which do not belong to our targeted products. Therefore, we improve the purity of the data by retrieving whether the feature *product_title* involves predefined keywords to accomplish useless reviews' removal. We demonstrated the comparison of the number of reviews before and after the cleaning in Table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c244e1",
   "metadata": {},
   "source": [
    "<center><b>Table 1</b>: comparison of the number of reviews before and after the cleaning.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635178b",
   "metadata": {},
   "source": [
    "|review number|hair-dryer|microwave|pacifier|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|before|11471|1616|18938|\n",
    "|after|11112|1604|11052|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7937f87",
   "metadata": {},
   "source": [
    "### 2.2 Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51988c14",
   "metadata": {},
   "source": [
    "Initially, we analyzed the variation of the average rating per year over time in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18559ed3",
   "metadata": {},
   "source": [
    "![](./image/1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ccb6a",
   "metadata": {},
   "source": [
    "**Figure 1**: A representative line chart showing that the average ratings of the three products fluctuate a lot from 2002 to 2009 with an increasing trend in general, and tend to converge to a stabilized average ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7766dcf",
   "metadata": {},
   "source": [
    "Representative words generated from the traning set with word-frequency are dipicted in Figure 2,3,4 with three kinds of products repectively. The bigger the size, the higher word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25cc3e",
   "metadata": {},
   "source": [
    "![](./image/2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b12f2",
   "metadata": {},
   "source": [
    "<center><b>(a) </b>Pacifier wordcloud map</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7024b",
   "metadata": {},
   "source": [
    "![](./image/3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566ebb9",
   "metadata": {},
   "source": [
    "<center><b>(b) </b>Microwave wordcloud map</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a930862",
   "metadata": {},
   "source": [
    "![](./image/4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacdd67f",
   "metadata": {},
   "source": [
    "<center><b>(c) </b>Hair-dryer wordcloud map</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace22d8",
   "metadata": {},
   "source": [
    "<b>Figure 2</b>: Wordcloud map based on generated words depicting high-frequency words of ecah product's review content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7094dc24",
   "metadata": {},
   "source": [
    "From the below star rating distribution chart we could that pacifier and hair dryer five-star rating accounted for the majority, while the microwave oven five-star and one-star rating accounted for the majority."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7940f0de",
   "metadata": {},
   "source": [
    "![](./image/5.jpg)\n",
    "![](./image/6.jpg)\n",
    "![](./image/7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ed1aa",
   "metadata": {},
   "source": [
    "<center><b>Figure 3</b>: Star rating distribution of pacifier, microwave and hair dryer based on given data.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad058f",
   "metadata": {},
   "source": [
    "## 3 Problem 1: Iteration-based Commodity Reputation Redistribution Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d125f9b",
   "metadata": {},
   "source": [
    "The parameters used in our analysis in this section are as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63136117",
   "metadata": {},
   "source": [
    "<center><b>Table 2: Key parameters for problem 1.</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b48945",
   "metadata": {},
   "source": [
    "|Symbol|Interpretation|\n",
    "|---|---|\n",
    "|$H_{1}$|the ratio of helpful and total votes of each review|\n",
    "|$H_{2}$|the intensity of emotion expression|\n",
    "|$H_{3}$|the ratio of attribute and total words of each review|\n",
    "|$H_{4}$|the ratio of adjectives and total words of each review|\n",
    "|$H_{5}$|the depth of each review|\n",
    "|$H_{6}$|whether the reviewer of the review is involved in Amazon Vine|\n",
    "|$H_{7}$|whether the reviewer of the review is verified customer|\n",
    "|$O$|the data set of all category of goods|\n",
    "|$N$|the number of data set $O$|\n",
    "|$W$|the confidence of each review|\n",
    "|$r$|the given rating of each review|\n",
    "|$C$|the data set of all customers|\n",
    "|$w$|the confidence of each reviewer|\n",
    "|$\\delta_{i}$|the confidence of each review content|\n",
    "|$t$|the timeliness of each review|\n",
    "|$\\theta$|a tunable parameter, which determines the weight of each review's reputation redistribution|\n",
    "|$TR$|the Pearson correlation coefficience|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f55471",
   "metadata": {},
   "source": [
    "Our comprehensive commodity evaluation model is built on the basis of the confidence of each review. Hence, we will define two fundamental metrics, product compasite rating $Q$ and review confidence $W_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a7fb5",
   "metadata": {},
   "source": [
    "### 3.1 the Defination of Commodity Reputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83013394",
   "metadata": {},
   "source": [
    "The most intuitionistic and straightforward method to confirm products' ranking could be taking their average ratings into account(we refer it as **Average Method**). Unfortunately, the method will be influenced by noise and artificial manipulation to a great extent due to the model's high sensitivity. In the rating system, we could reasonably infer that some users may comment with unreasonable ratings with no reason because they are not serious about the rating or benefit from fake five-star rating instead of rating that reflect their real aspiration. Thus we refer to the method derived from the article[1], finally we define the reputation of product $Q$ as:\n",
    "\n",
    "\n",
    "$$Q =\\frac{\\sum{Q_j}}{N}$$\n",
    "\n",
    "\n",
    "$$Q_j=max\\left\\{W_i\\right\\}\\frac{\\sum{r_i*W_i}}{\\sum{W_i}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $O$ denotes the data set of all categories of goods.\n",
    "\n",
    "\n",
    "* $N=|O|$, denotes the number of data set $O$.\n",
    "\n",
    "\n",
    "* $W_{i}$ denotes the confidence of each review $i$.\n",
    "\n",
    "\n",
    "* $r_{i}$ denotes the giving rating of each review $i$.\n",
    "\n",
    "\n",
    "* $C$ denotes the data set of all customers.\n",
    "\n",
    "\n",
    "Since different reviewers and contents would have an impact on the trustworthiness of these themselves, we import the defination of review confidence. Compared to the simple and unstablized average rating method which indicates the quality of each product, we assign generated weight to each review in the calculation of the ratings, in this case we could greatly increase the accracy and robustness of the quality rating[2].\n",
    "\n",
    "\n",
    "In order to deter from certain users' arbitrary attitudes toward the star rating[1], we introduce our **Commodity Reputation Redistribution Model**. What's more, in our hypothesis, we believe that there is a intense correlation between sales volume and the number of reviews. Thus we added penalty factors to the original weighted average equation. By this means, the ratings with few reviews are reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3e642",
   "metadata": {},
   "source": [
    "### 3.2 the Defination of Customer Reputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4465e2d6",
   "metadata": {},
   "source": [
    "It can be reasonably inferred that if the confidence of a comment consists of the comment text content and the timeliness of each review[3]. Therefore, we define the comment confidence as:\n",
    "\n",
    "\n",
    "$$W_i=w_i*\\delta_i*t_i(i\\in C_\\alpha)$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "* $w_i$ denotes the confidence of reviewer.\n",
    "\n",
    "\n",
    "* $\\delta_i$ denoted the confidence of the review text content.\n",
    "\n",
    "\n",
    "* $t_i$ denotes the timeliness of review.\n",
    "\n",
    "\n",
    "* $C_\\alpha$ denotes the data set of product $\\alpha$' reviewers.\n",
    "\n",
    "\n",
    "Then, in below three sections, we will state the exact defination of $w,\\delta ,t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a185d5e",
   "metadata": {},
   "source": [
    "#### 3.2.1 the Defination of Reviewwe Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534d83b",
   "metadata": {},
   "source": [
    "Generally speaking, we trust that the content of the comments is credible and unbiased. Therfore, we select the **Pearson correlation coefficience** between the reviewer's score and the item's true rating as the critical factor of our model. On the other hand, if the reviewer submitted few reviews, e.g only ome or two, then we dislodge the skeptical decision even if his or her Pearson correlation coefficience is dramatically high. In consequence, we are introduced to employ penalty factors that are applied to reduce the reputation scores of those reviewers stingy with comments.\n",
    "\n",
    "\n",
    "Each customer's Pearson correlation coefficience is showed below:\n",
    "\n",
    "\n",
    "$$TR_i=\\frac{lg(k_i)}{max\\left\\{ k_j \\right\\}} \\cdot \\frac{1}{k_i}\\cdot\\sum (\\frac{r_{i\\alpha}-\\overline{r_{i}}}{\\sigma_{r_i}})\\cdot(\\frac{Q_{\\alpha}-\\overline{Q_{i}}}{\\sigma_{Q_i}})(i\\in C_\\alpha ,j\\in C,\\alpha \\in O)$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "* $k$ denotes the number of posted review geared to each reviewer.\n",
    "\n",
    "\n",
    "* $r_i\\alpha$  denotes reviewer $i$ to the product $\\alpha$.\n",
    "\n",
    "\n",
    "* $\\theta$ denotes the SD(standard deviation).\n",
    "\n",
    "\n",
    "Although we aquired Pearson correlation coefficience from each customer, we cannot yet use it as a credibel level of each customer. For the sake of generating higher values from those with higher $TR$ values, we are required to supplement the procedure of reputation. The core process is defined by the equation below:\n",
    "\n",
    "\n",
    "$$w_i=TR_i^\\theta\\frac{\\sum TR_j}{\\sum TR_j^\\theta}$$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "* $\\theta$ a tunable parameter, which determines the weight of each review's reputation redistribution.\n",
    "\n",
    "\n",
    "* $TR$ denotes the Pearson correlation coefficience.\n",
    "\n",
    "\n",
    "And we consider the value of $w_i$ as the index to measure the informativeness of the data measures based on ratings and reviews, in this way we downsize the data set into almost 1% of the original data set. We abstracted select reviews and demonstrated the comparison of the number of reviews before and after the data abstraction in **Table 3**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1a29f",
   "metadata": {},
   "source": [
    "<center><b>Table 3: </b>Comparison of the number of reviews before and after</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f0c5f",
   "metadata": {},
   "source": [
    "|review number|hair-dryer|microwave|pacifier|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "|original|11112|1604|11052|\n",
    "|most informative|137|9|477|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee54cea",
   "metadata": {},
   "source": [
    "#### 3.2.2 the Defination of Text Content Confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a63a9",
   "metadata": {},
   "source": [
    "For each comment, we are supposed to convert its content into normalized accountale value. We taken four features into account, which are all emerged in **Table 4**. Therfore, we consider applying the **Fuzzy Analytic Hierarchy Processing(FAHP)** to figure up the weights of each feature and make use of **Technique for Order Preference by Similarity to Ideal Solution(TOPSIS)** to determine the final credibal level of each review. It's worth noting that the depth of the review has been processed via **Laplacian correction** for the sake of avoiding the specific situation that information carried by other attributes being wiped off by the attribute values not ever presented in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09895846",
   "metadata": {},
   "source": [
    "<center><b>Table 4: </b>Feature Table</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a14dd9",
   "metadata": {},
   "source": [
    "|Symbol|Defination|Interpretation|\n",
    "|:---|:---:|:---|\n",
    "|$H_{1}$|$\\frac{helpful\\_vote}{total\\_vote}$|the ratio of helpful and total votes of each review|\n",
    "|$H_{2}$|**None**|the intensity of emotion expression|\n",
    "|$H_{3}$|$\\frac{N_a}{N_t}$|the ratio of attribute and total words of each review|\n",
    "|$H_{4}$|$\\frac{N_b}{N_t}$|the ratio of adjectives and total words of each review|\n",
    "|$H_{5}$|$\\frac{ln(N_a+N_b)}{ln(N_t) + 1}$|the depth of each review|\n",
    "|$H_{6}$|*0* $or$ *1*|whether the reviewer of the review is involved in Amazon Vine|\n",
    "|$H_{7}$|*0* $or$ *1*|whether the reviewer of the review is verified customer|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57c7ed",
   "metadata": {},
   "source": [
    "Fuzzy hierarchical analysis is a quantitive analysis method that combines hierarchical analysis with fuzzy comprehensive evaluation. In this litarature, **FAHP** metheod is used to determine the weights. The data for the critical matrix judgement in the FAHP is generated from expert scoring, the specific method is as followed.\n",
    "\n",
    "\n",
    "Respondents were asked to score two-by-two comparisons according to the importance factors influencing customers' shopping decisions-making, and each pair of attribute comparison items was scaled by [0.1,0.9], and users were asked to filtered out the importance relationship matrix by comparing the importance of different factors. The importance relationship is expressed as an examination function $f(x,y)$, which represents the importance scale of factor x and factor y for the overall, and the importance matrix for $f(x,y)$ is constructed by applying the **list comparison method**. With the list comparison method to construct the priority relation matrix, the targeted scales are depicted in **Table 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be3c75",
   "metadata": {},
   "source": [
    "<center><b>Table 5</b>: Measures of Factors' Importance</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e497d2",
   "metadata": {},
   "source": [
    "|Defination|Interpretation|Measure|\n",
    "|:---|---|:---|\n",
    "|Equally important|x and y are equally important|0.5|\n",
    "|Slightly important|x is slightly important|0.6|\n",
    "|Obviously important|x is obviously important|0.7|\n",
    "|Special important|x is special important|0.8|\n",
    "|Extreme important|x is extreme important|0.9|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369bd0ba",
   "metadata": {},
   "source": [
    "Then we will construct the matrix of determination $C=(c_{ij})_{n_\\times m},$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "C=\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    " C_{11}       & C_{12}       & \\cdots &  C_{1n}       \\\\\n",
    " C_{21}       & C_{22}      & \\cdots &  C_{2n}       \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " C_{n1}       & C_{n2}       & \\cdots &  C_{nn}       \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    ",(c_{ij}=0.5)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eadbe5",
   "metadata": {},
   "source": [
    "Normalize the elements of the determination matrix.\n",
    "\n",
    "where:\n",
    "\n",
    "$$B=(b_{ij})_{m\\times n}$$\n",
    "\n",
    "$$b_{ij}=\\frac{a_{ij}}{\\sum_{i=1}^{n}a_{ij}},(i,j=1,2,...,n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f930647",
   "metadata": {},
   "source": [
    "Then, the elements in matrix $B$ are summed by rows to obtain vector $C=(c_1,c_2,...,c_n)^T$ \n",
    "\n",
    "where:\n",
    "\n",
    "$$c_ij=\\sum_{i=1}^{n}a_{ij},(i,j=1,2,...,n)$$\n",
    "\n",
    "Finally, we would normalize the vector $C$ to obtain the eigenvector $W=\\left\\{w_i,w_i,...,w_n\\right\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af479bcb",
   "metadata": {},
   "source": [
    "In this literature ,the reformative **TOPSIS** judging method is applied as online review usefulness ranking filtering model algorithm. The idea is on the basis of determining the weights of each attribute index, together with normalizing the original data matrix, calculating the distance among each evaluation , especially within the optimal and the worst solution, then obtaining the relative proximity of each evaluation to identify the priority of each factors. The specific algorithm steps are as followed.\n",
    "\n",
    "1. In order to eliminate the magnitude effect among different attributes and make each attribute equally expressive, the raw data has been initially normalized. We define the matrix of the multi-attribute decision matrix as:\n",
    "\n",
    "$$A=(a_{ij})_{m\\times n}$$\n",
    "\n",
    "$$b_{ij}=\\frac{a_{ij}-\\bar{a_j}}{s_j},i=1,2,...,m;j=1,2,...,n$$\n",
    "\n",
    "\n",
    "2. In order to optimize our calculation, we introduce a weighted specification matrix $C_W=(c_{ij}^w)_{m\\times n}$\n",
    "\n",
    "\n",
    "3. Calculate the **Euclidean Distance** from each alternative to the positive ideal solution and the negative ideal solution. \n",
    "\n",
    "The diatance of the alternative $d_i$ to the positive ideal solution:\n",
    "\n",
    "$$s^{+}=\\sqrt{\\sum_{i=1}^{m}(c_{ij}-c_{j}^{+})^2},i=1,2,...,m$$\n",
    "\n",
    "The diatance of the alternative $d_i$ to the negative ideal solution:\n",
    "\n",
    "$$s^{-}=\\sqrt{\\sum_{i=1}^{m}(c_{ij}-c_{j}^{-})^2},i=1,2,...,m$$\n",
    "\n",
    "\n",
    "4. Calculate the queuing index value i.e., the composite evaluation index for each content confidence:\n",
    "\n",
    "$$\\delta _i=\\frac{s_i^-}{s_i^-+s_i^+}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999754bf",
   "metadata": {},
   "source": [
    "#### 3.2.3 the Defination of Timeliness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163e60b",
   "metadata": {},
   "source": [
    "The timimg of the comment delivery is as well as one of the most critical factors that supposed to be as a credit measurement. The longer the comment has been posted, the less convinciable it would be. Otherwise, we could make an assumption that The comments the users have initially seen are the latest. We also observed that the curve of the timeliness and interval of comment data are not seriously linear as expected. Therefore, we are intended to introduce an **Exponential Function** as a basic function for timeliness measurement:\n",
    "\n",
    "$$t_i=e^{\\bigtriangleup t}$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\bigtriangleup t$ is the interval between the data to initially read the latest comment and the data to post thier own review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a5da2",
   "metadata": {},
   "source": [
    "#### 3.2.4 Iterative method to solve the convergence value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4ce69",
   "metadata": {},
   "source": [
    "The **HITS** algorithm with iterative refinement procedure is a link-based model, which was primitively applied to identify the hubs pages that linked to multiple related authorities. Since there is mutually reinforcing relationship between the hubs and the authorities, while the iterative algorithm which can maintain and update real-time numerical weights for each pages. Specifically, the relationship between the hubs and the authorities is described by the formula as followed.\n",
    "\n",
    "Firstly, the weights of the hub $i$ and the authority $\\alpha$ are respectively initialized as $x^{(i)}$ and $y^{(\\alpha)}$ after normalization. Then, the weight $x^{(i)}$ of the hub is updated by:\n",
    "\n",
    "$$x^{(i)}\\gets\\sum{}{} y^{(\\alpha)}$$\n",
    "\n",
    "where $y^{(\\alpha)}$ denotes the set of the authorities that pointed by the hub $i$. On the contrary, the weight $y^{(\\alpha)}$ of the authority is updated by:\n",
    "\n",
    "$$y^{(\\alpha)}\\to \\sum{}{}x^{(i)}$$\n",
    "\n",
    "This algorithm could also be devoted to accomplish $W$ and $Q$ convergence eventually by iterating continuously. The integrative computation of the ratings of the product and the reputation of the reviewer are ideally solved[4].\n",
    "\n",
    "At the bottom of this section we geberated the time-based comprehensive score line chart in **Figure 4**.\n",
    "\n",
    "![](./image/8.jpg)\n",
    "\n",
    "<b>Figure 4</b>: The rising tendency of the reputation of three products over time indicates their broad market prospects are being valued by the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68209e21",
   "metadata": {},
   "source": [
    "## Problem 3: the Successfulness Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b7c9d",
   "metadata": {},
   "source": [
    "**Simple Exponential Smoothing(SES)** also called **Linear Exponential Smoothing**, always applied for time series analysis for discret series with smooth slope or convergence. From the section above, it is not hard to observe that the ratings have gradually converged to a constant. Thus, nothing is better than employing SES as the solution for our outcome from section above to predict the reputation of the produncts' trend.\n",
    "\n",
    "Firstly, we assume our original series as $y_1,y_2,...,y_t$, and a constant $\\alpha$.\n",
    "\n",
    "Besides, $S_t^{(1)}$ is the predicted value of the series $S$ at moment $t$.\n",
    "\n",
    "On the basis of the assumptions above, we could make the defination:\n",
    "\n",
    "$$S_t^{(1)}=\\alpha y_t+(1-\\alpha)S_{t-1}^(1)=S_{t-1}^{(1)}+\\alpha (y_t-S_{t-1}^{(1)})$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$S_t^{(1)}=\\alpha \\sum_{j=0}^\\infty (1-\\alpha )^jy_{t-j}$$\n",
    "\n",
    "Eventually we could get the equation to accomplish prediction:\n",
    "\n",
    "$$\\hat{y}_{t+1}=S_t^{(1)}=\\alpha y_t+(1-\\alpha )\\hat{y}_t$$\n",
    "\n",
    "We generated the time based comprehensive prediction line chart in **Figure 5**.\n",
    "\n",
    "![](./image/9.jpg)\n",
    "\n",
    "<b>Figure 5</b>: the result of the reputation $Q$ projections reveal that three products' marketting are proved to be potentially successful and recommended for further investment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe7a537",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47dfb0",
   "metadata": {},
   "source": [
    "[1] Hao Liao, An Zeng, Rui Xiao, Zhuo-Ming Ren, Duan-Bing Chen, and Yi-Cheng Zhang.\n",
    "Ranking reputation and quality in online rating systems. PloS one, 9(5):e97146, 2014.\n",
    "\n",
    "\n",
    "[2]Chien Chin Chen andYou-DeTseng. Quality evaluation of product reviews using an information\n",
    "quality framework. Decision Support Systems, 50(4):755–768, 2011.\n",
    "\n",
    "\n",
    "[3]Samita Dhanasobhon, Pei-Yu Chen, Michael Smith, and Pei-yu Chen. An analysis of the\n",
    "differential impact of reviews and reviewers at amazon. com. ICIS 2007 Proceedings, page 94,\n",
    "2007.\n",
    "\n",
    "\n",
    "[4]Jian Gao and Tao Zhou. Evaluating user reputation in online rating systems via an iterative\n",
    "group-based ranking method. Physica A: Statistical Mechanics and its Applications, 473:546–\n",
    "560, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c871c9d",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b9bf9",
   "metadata": {},
   "source": [
    "## Appendix Source Code for FAHP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "A1 = [0, 0, 0, 0, 0, 0]  # 记录模糊互补矩阵每行和/Record the sum of each row of the fuzzy complementary matrix\n",
    "R1 = [0, 0, 0, 0, 0, 0]  # 记录一致性矩阵每行积/Record the product of each row of the consistency matrix\n",
    "W = [0, 0, 0, 0, 0, 0]\n",
    "R = np.zeros((6, 6), dtype=np.float)\n",
    "SUM = 0\n",
    "global N\n",
    "N = 6\n",
    "\n",
    "# 模糊互补矩阵/Fuzzy Complementary Matrix\n",
    "A = np.array([\n",
    "    [0.50, 0.75, 0.80, 0.60, 0.50, 0.55],\n",
    "    [0.25, 0.50, 0.65, 0.50, 0.30, 0.40],\n",
    "    [0.20, 0.35, 0.50, 0.40, 0.30, 0.40],\n",
    "    [0.40, 0.50, 0.60, 0.50, 0.30, 0.40],\n",
    "    [0.50, 0.70, 0.70, 0.70, 0.50, 0.75],\n",
    "    [0.45, 0.60, 0.60, 0.60, 0.25, 0.50]\n",
    "])\n",
    "\n",
    "for i in range(N):\n",
    "    A1[i] = 0  # 记录每行的和/Record the sum of each row\n",
    "    for j in range(N):\n",
    "        A1[i] += A[i][j]\n",
    "print(A1)\n",
    "\n",
    "# 转换成模糊一致性矩阵/Convert to Fuzzy Consistency Matrix\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        R[i][j] = (A1[i] - A1[j]) / (2 * N) + 0.5\n",
    "        \n",
    "# 幂积法求单层权重/Power product method to find single layer weights\n",
    "for i in range(N):\n",
    "    R1[i] = 1\n",
    "    for j in range(N):\n",
    "        R1[i] *= R[i][j]\n",
    "    W[i] = pow(R1[i], 0.2)\n",
    "    SUM += W[i]\n",
    "    \n",
    "for i in range(N):\n",
    "    W[i] = W[i] / SUM\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c0883e",
   "metadata": {},
   "source": [
    "## Appendix Source Code for TOPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topsis(data, weight=None):\n",
    "    data = data / np.sqrt((data ** 2).sum())  # normalized\n",
    "\n",
    "    Z = pd.DataFrame([data.min(), data.max()], index=['负理想解/negative ideal solution', '正理想解/positive ideal solution'])  # best and worst solution\n",
    "\n",
    "    weight = entropy_weight(data) if weight is None else np.array(weight)  # distance\n",
    "    Result = data.copy()\n",
    "    Result['正理想解'] = np.sqrt(((data - Z.loc['正理想解']) ** 2 * weight).sum(axis=1))\n",
    "    Result['负理想解'] = np.sqrt(((data - Z.loc['负理想解']) ** 2 * weight).sum(axis=1))\n",
    "\n",
    "    # composite score index\n",
    "    Result['综合得分指数/composite score index'] = Result['负理想解'] / (Result['负理想解'] + Result['正理想解'])\n",
    "    Result['sequence'] = Result.rank(ascending=False)['综合得分指数']\n",
    "\n",
    "    return Result, Z, weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "99.9958px",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
